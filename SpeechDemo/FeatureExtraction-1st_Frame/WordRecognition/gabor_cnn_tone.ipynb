{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.utils import plot_model,to_categorical\n",
    "from keras import Model,Input,regularizers\n",
    "from keras.layers import Reshape,Flatten\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras.backend as K\n",
    "import keras.callbacks as kcallbacks\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import string\n",
    "from keras.optimizers import SGD\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "# from lsuv_init import LSUVinit\n",
    "warnings.filterwarnings('ignore')\n",
    "from tensorflow.keras import layers\n",
    "from keras.layers import Input, Dense, LSTM, merge ,Conv1D,Dropout,Bidirectional,Multiply,BatchNormalization,MaxPool1D\n",
    "from keras.models import Model\n",
    "import time\n",
    "\n",
    "from attention_utils import get_activations\n",
    "from keras.layers import merge\n",
    "from keras.layers.core import *\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import *\n",
    "\n",
    "import  pandas as pd\n",
    "import  numpy as np\n",
    "from keras.wrappers import scikit_learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SINGLE_ATTENTION_VECTOR = False\n",
    "def attention_3d_block(inputs, single_attention_vector=False):\n",
    "    # 如果上一层是LSTM，需要return_sequences=True\n",
    "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
    "    time_steps = K.int_shape(inputs)[1]\n",
    "    input_dim = K.int_shape(inputs)[2]\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    a = Dense(time_steps, activation='softmax', name='attention')(a)\n",
    "    if single_attention_vector:\n",
    "        a = Lambda(lambda x: K.mean(x, axis=1))(a)\n",
    "        a = RepeatVector(input_dim)(a)\n",
    "\n",
    "    a_probs = Permute((2, 1))(a)\n",
    "    # 乘上了attention权重，但是并没有求和，好像影响不大\n",
    "    # 如果分类任务，进行Flatten展开就可以了\n",
    "    # element-wise\n",
    "    output_attention_mul = Multiply()([inputs, a_probs])\n",
    "    return output_attention_mul\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features=np.load('X_gabor.npy')\n",
    "y=np.load('y_tone.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4704, 35, 7)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = np.mean(X_features, axis=0)\n",
    "X_features -= means\n",
    "stds = np.std(X_features, axis=0)\n",
    "X_features /= stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# means = np.mean(X_features, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "le=LabelEncoder()\n",
    "y=le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 3 3 ... 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1' '2' '3' '4']\n"
     ]
    }
   ],
   "source": [
    "CLASSES_LIST=le.classes_\n",
    "print(CLASSES_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=to_categorical(y,num_classes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4704, 4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y=y.reshape(4704,1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4704, 4)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(116)\n",
    "np.random.shuffle(X_features)\n",
    "np.random.seed(116)\n",
    "np.random.shuffle(y)\n",
    "tf.random.set_seed(116)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test=train_test_split(X_features,y,test_size=0.2,random_state=2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3763, 35, 7), (941, 35, 7))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES=4\n",
    "BATCH_SIZE=32\n",
    "EPOCHS=100\n",
    "MODEL_SAVE_PATH='.\\model\\gabor_tone_cnn1.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layers.Dense(1, activation=\"sigmoid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_model():\n",
    "    inputs = Input(shape=(35, 7))\n",
    "    #x=BatchNormalization()(inputs)\n",
    "    x = Conv1D(filters = 64, kernel_size = 1, padding=\"same\",kernel_initializer='he_normal',\n",
    "                bias_initializer='zeros',activation = 'relu')(inputs)  #, padding = 'same'\n",
    "    x = MaxPool1D()(x)\n",
    "    #lstm_out = Bidirectional(LSTM(lstm_units, activation='relu'), name='bilstm')(x)\n",
    "    #对于GPU可以使用CuDNNLSTM\n",
    "    lstm_out = Bidirectional(LSTM(128, return_sequences=True,dropout=0.4))(x)\n",
    "    lstm_out = Dropout(0.3)(lstm_out)\n",
    "    attention_mul = attention_3d_block(lstm_out)\n",
    "    attention_mul = Flatten()(attention_mul)\n",
    "    output = Dense(4, activation='sigmoid')(attention_mul)\n",
    "    model = Model(inputs=[inputs], outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 35, 7)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 35, 64)       512         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 17, 64)       0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 17, 256)      197632      max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 17, 256)      0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "permute_4 (Permute)             (None, 256, 17)      0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention (Dense)               (None, 256, 17)      306         permute_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "permute_5 (Permute)             (None, 17, 256)      0           attention[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, 17, 256)      0           dropout_2[0][0]                  \n",
      "                                                                 permute_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 4352)         0           multiply_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 4)            17412       flatten_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 215,862\n",
      "Trainable params: 215,862\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = attention_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pydotplus\n",
    "# import graphviz \n",
    "# from keras.utils import plot_model\n",
    "# plot_model(model, to_file='.\\\\Vedio_tone.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "adam=optimizers.Adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "91/95 [===========================>..] - ETA: 0s - loss: 1.3009 - accuracy: 0.3836\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.40372, saving model to .\\model\\gabor_tone_cnn1.h5\n",
      "95/95 [==============================] - 2s 18ms/step - loss: 1.3015 - accuracy: 0.3850 - val_loss: 1.2564 - val_accuracy: 0.4037\n",
      "Epoch 2/100\n",
      "90/95 [===========================>..] - ETA: 0s - loss: 1.2389 - accuracy: 0.4236\n",
      "Epoch 00002: val_accuracy improved from 0.40372 to 0.43028, saving model to .\\model\\gabor_tone_cnn1.h5\n",
      "95/95 [==============================] - 1s 12ms/step - loss: 1.2407 - accuracy: 0.4199 - val_loss: 1.2410 - val_accuracy: 0.4303\n",
      "Epoch 3/100\n",
      "92/95 [============================>.] - ETA: 0s - loss: 1.2142 - accuracy: 0.4399\n",
      "Epoch 00003: val_accuracy improved from 0.43028 to 0.43161, saving model to .\\model\\gabor_tone_cnn1.h5\n",
      "95/95 [==============================] - 1s 12ms/step - loss: 1.2155 - accuracy: 0.4389 - val_loss: 1.2209 - val_accuracy: 0.4316\n",
      "Epoch 4/100\n",
      "93/95 [============================>.] - ETA: 0s - loss: 1.1964 - accuracy: 0.4479\n",
      "Epoch 00004: val_accuracy improved from 0.43161 to 0.44090, saving model to .\\model\\gabor_tone_cnn1.h5\n",
      "95/95 [==============================] - 1s 12ms/step - loss: 1.1954 - accuracy: 0.4488 - val_loss: 1.2113 - val_accuracy: 0.4409\n",
      "Epoch 5/100\n",
      "92/95 [============================>.] - ETA: 0s - loss: 1.1738 - accuracy: 0.4637\n",
      "Epoch 00005: val_accuracy did not improve from 0.44090\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 1.1761 - accuracy: 0.4654 - val_loss: 1.2131 - val_accuracy: 0.4250\n",
      "Epoch 6/100\n",
      "93/95 [============================>.] - ETA: 0s - loss: 1.1668 - accuracy: 0.4664\n",
      "Epoch 00006: val_accuracy improved from 0.44090 to 0.44489, saving model to .\\model\\gabor_tone_cnn1.h5\n",
      "95/95 [==============================] - 1s 13ms/step - loss: 1.1660 - accuracy: 0.4671 - val_loss: 1.2018 - val_accuracy: 0.4449\n",
      "Epoch 7/100\n",
      "93/95 [============================>.] - ETA: 0s - loss: 1.1465 - accuracy: 0.4788\n",
      "Epoch 00007: val_accuracy improved from 0.44489 to 0.45153, saving model to .\\model\\gabor_tone_cnn1.h5\n",
      "95/95 [==============================] - 1s 13ms/step - loss: 1.1489 - accuracy: 0.4774 - val_loss: 1.1925 - val_accuracy: 0.4515\n",
      "Epoch 8/100\n",
      "92/95 [============================>.] - ETA: 0s - loss: 1.1385 - accuracy: 0.4895\n",
      "Epoch 00008: val_accuracy improved from 0.45153 to 0.45286, saving model to .\\model\\gabor_tone_cnn1.h5\n",
      "95/95 [==============================] - 1s 13ms/step - loss: 1.1368 - accuracy: 0.4914 - val_loss: 1.1967 - val_accuracy: 0.4529\n",
      "Epoch 9/100\n",
      "91/95 [===========================>..] - ETA: 0s - loss: 1.1191 - accuracy: 0.4962\n",
      "Epoch 00009: val_accuracy improved from 0.45286 to 0.46215, saving model to .\\model\\gabor_tone_cnn1.h5\n",
      "95/95 [==============================] - 1s 13ms/step - loss: 1.1206 - accuracy: 0.4937 - val_loss: 1.1898 - val_accuracy: 0.4622\n",
      "Epoch 10/100\n",
      "90/95 [===========================>..] - ETA: 0s - loss: 1.1064 - accuracy: 0.5097\n",
      "Epoch 00010: val_accuracy improved from 0.46215 to 0.47012, saving model to .\\model\\gabor_tone_cnn1.h5\n",
      "95/95 [==============================] - 1s 13ms/step - loss: 1.1086 - accuracy: 0.5076 - val_loss: 1.1883 - val_accuracy: 0.4701\n",
      "Epoch 11/100\n",
      "90/95 [===========================>..] - ETA: 0s - loss: 1.0889 - accuracy: 0.5139\n",
      "Epoch 00011: val_accuracy did not improve from 0.47012\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 1.0921 - accuracy: 0.5163 - val_loss: 1.1910 - val_accuracy: 0.4568\n",
      "Epoch 12/100\n",
      "92/95 [============================>.] - ETA: 0s - loss: 1.0828 - accuracy: 0.5160\n",
      "Epoch 00012: val_accuracy did not improve from 0.47012\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 1.0814 - accuracy: 0.5183 - val_loss: 1.1955 - val_accuracy: 0.4555\n",
      "Epoch 13/100\n",
      "91/95 [===========================>..] - ETA: 0s - loss: 1.0594 - accuracy: 0.5316\n",
      "Epoch 00013: val_accuracy did not improve from 0.47012\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 1.0666 - accuracy: 0.5276 - val_loss: 1.1955 - val_accuracy: 0.4555\n",
      "Epoch 14/100\n",
      "91/95 [===========================>..] - ETA: 0s - loss: 1.0505 - accuracy: 0.5426\n",
      "Epoch 00014: val_accuracy did not improve from 0.47012\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 1.0494 - accuracy: 0.5429 - val_loss: 1.2076 - val_accuracy: 0.4475\n",
      "Epoch 15/100\n",
      "91/95 [===========================>..] - ETA: 0s - loss: 1.0387 - accuracy: 0.5388\n",
      "Epoch 00015: val_accuracy improved from 0.47012 to 0.49270, saving model to .\\model\\gabor_tone_cnn1.h5\n",
      "95/95 [==============================] - 1s 13ms/step - loss: 1.0414 - accuracy: 0.5359 - val_loss: 1.1896 - val_accuracy: 0.4927\n",
      "Epoch 16/100\n",
      "90/95 [===========================>..] - ETA: 0s - loss: 1.0285 - accuracy: 0.5479\n",
      "Epoch 00016: val_accuracy did not improve from 0.49270\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 1.0276 - accuracy: 0.5482 - val_loss: 1.2095 - val_accuracy: 0.4847\n",
      "Epoch 17/100\n",
      "91/95 [===========================>..] - ETA: 0s - loss: 1.0180 - accuracy: 0.5573\n",
      "Epoch 00017: val_accuracy did not improve from 0.49270\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 1.0160 - accuracy: 0.5598 - val_loss: 1.1912 - val_accuracy: 0.4688\n",
      "Epoch 18/100\n",
      "92/95 [============================>.] - ETA: 0s - loss: 0.9870 - accuracy: 0.5713\n",
      "Epoch 00018: val_accuracy did not improve from 0.49270\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 0.9873 - accuracy: 0.5721 - val_loss: 1.2071 - val_accuracy: 0.4701\n",
      "Epoch 19/100\n",
      "91/95 [===========================>..] - ETA: 0s - loss: 0.9915 - accuracy: 0.5663\n",
      "Epoch 00019: val_accuracy did not improve from 0.49270\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 0.9903 - accuracy: 0.5674 - val_loss: 1.1932 - val_accuracy: 0.4900\n",
      "Epoch 20/100\n",
      "91/95 [===========================>..] - ETA: 0s - loss: 0.9660 - accuracy: 0.5780\n",
      "Epoch 00020: val_accuracy did not improve from 0.49270\n",
      "95/95 [==============================] - 1s 12ms/step - loss: 0.9696 - accuracy: 0.5777 - val_loss: 1.2049 - val_accuracy: 0.4768\n",
      "Epoch 21/100\n",
      "91/95 [===========================>..] - ETA: 0s - loss: 0.9628 - accuracy: 0.5876\n",
      "Epoch 00021: val_accuracy did not improve from 0.49270\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 0.9605 - accuracy: 0.5897 - val_loss: 1.1775 - val_accuracy: 0.4874\n",
      "Epoch 22/100\n",
      "91/95 [===========================>..] - ETA: 0s - loss: 0.9302 - accuracy: 0.6147\n",
      "Epoch 00022: val_accuracy did not improve from 0.49270\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 0.9274 - accuracy: 0.6156 - val_loss: 1.1992 - val_accuracy: 0.4914\n",
      "Epoch 23/100\n",
      "91/95 [===========================>..] - ETA: 0s - loss: 0.9171 - accuracy: 0.6106\n",
      "Epoch 00023: val_accuracy improved from 0.49270 to 0.49668, saving model to .\\model\\gabor_tone_cnn1.h5\n",
      "95/95 [==============================] - 1s 13ms/step - loss: 0.9166 - accuracy: 0.6130 - val_loss: 1.2100 - val_accuracy: 0.4967\n",
      "Epoch 24/100\n",
      "91/95 [===========================>..] - ETA: 0s - loss: 0.9237 - accuracy: 0.6082\n",
      "Epoch 00024: val_accuracy did not improve from 0.49668\n",
      "95/95 [==============================] - 1s 12ms/step - loss: 0.9214 - accuracy: 0.6093 - val_loss: 1.1996 - val_accuracy: 0.4914\n",
      "Epoch 25/100\n",
      "91/95 [===========================>..] - ETA: 0s - loss: 0.8904 - accuracy: 0.6178\n",
      "Epoch 00025: val_accuracy improved from 0.49668 to 0.49801, saving model to .\\model\\gabor_tone_cnn1.h5\n",
      "95/95 [==============================] - 1s 13ms/step - loss: 0.8875 - accuracy: 0.6199 - val_loss: 1.2136 - val_accuracy: 0.4980\n",
      "Epoch 26/100\n",
      "90/95 [===========================>..] - ETA: 0s - loss: 0.8725 - accuracy: 0.6299\n",
      "Epoch 00026: val_accuracy did not improve from 0.49801\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 0.8752 - accuracy: 0.6289 - val_loss: 1.2409 - val_accuracy: 0.4887\n",
      "Epoch 27/100\n",
      "91/95 [===========================>..] - ETA: 0s - loss: 0.8533 - accuracy: 0.6380\n",
      "Epoch 00027: val_accuracy improved from 0.49801 to 0.50199, saving model to .\\model\\gabor_tone_cnn1.h5\n",
      "95/95 [==============================] - 1s 13ms/step - loss: 0.8536 - accuracy: 0.6369 - val_loss: 1.2377 - val_accuracy: 0.5020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/100\n",
      "95/95 [==============================] - ETA: 0s - loss: 0.8468 - accuracy: 0.6415\n",
      "Epoch 00028: val_accuracy did not improve from 0.50199\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 0.8468 - accuracy: 0.6415 - val_loss: 1.2282 - val_accuracy: 0.4794\n",
      "Epoch 29/100\n",
      "90/95 [===========================>..] - ETA: 0s - loss: 0.8241 - accuracy: 0.6653\n",
      "Epoch 00029: val_accuracy did not improve from 0.50199\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 0.8255 - accuracy: 0.6645 - val_loss: 1.2332 - val_accuracy: 0.4874\n",
      "Epoch 30/100\n",
      "91/95 [===========================>..] - ETA: 0s - loss: 0.8154 - accuracy: 0.6672\n",
      "Epoch 00030: val_accuracy did not improve from 0.50199\n",
      "95/95 [==============================] - 1s 12ms/step - loss: 0.8167 - accuracy: 0.6661 - val_loss: 1.2389 - val_accuracy: 0.4648\n",
      "Epoch 31/100\n",
      "93/95 [============================>.] - ETA: 0s - loss: 0.7917 - accuracy: 0.6640\n",
      "Epoch 00031: val_accuracy did not improve from 0.50199\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 0.7957 - accuracy: 0.6618 - val_loss: 1.2736 - val_accuracy: 0.4754\n",
      "Epoch 32/100\n",
      "91/95 [===========================>..] - ETA: 0s - loss: 0.7700 - accuracy: 0.6837\n",
      "Epoch 00032: val_accuracy did not improve from 0.50199\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 0.7732 - accuracy: 0.6827 - val_loss: 1.2998 - val_accuracy: 0.4847\n",
      "Epoch 33/100\n",
      "92/95 [============================>.] - ETA: 0s - loss: 0.7653 - accuracy: 0.6926\n",
      "Epoch 00033: val_accuracy did not improve from 0.50199\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 0.7630 - accuracy: 0.6934 - val_loss: 1.2884 - val_accuracy: 0.4834\n",
      "Epoch 34/100\n",
      "92/95 [============================>.] - ETA: 0s - loss: 0.7364 - accuracy: 0.7126\n",
      "Epoch 00034: val_accuracy did not improve from 0.50199\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 0.7391 - accuracy: 0.7103 - val_loss: 1.2736 - val_accuracy: 0.4754\n",
      "Epoch 35/100\n",
      "91/95 [===========================>..] - ETA: 0s - loss: 0.7150 - accuracy: 0.7163\n",
      "Epoch 00035: val_accuracy did not improve from 0.50199\n",
      "95/95 [==============================] - 1s 12ms/step - loss: 0.7117 - accuracy: 0.7176 - val_loss: 1.2695 - val_accuracy: 0.4967\n",
      "Epoch 36/100\n",
      "91/95 [===========================>..] - ETA: 0s - loss: 0.7105 - accuracy: 0.7098\n",
      "Epoch 00036: val_accuracy did not improve from 0.50199\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 0.7141 - accuracy: 0.7066 - val_loss: 1.3381 - val_accuracy: 0.4768\n",
      "Epoch 37/100\n",
      "91/95 [===========================>..] - ETA: 0s - loss: 0.6903 - accuracy: 0.7205\n",
      "Epoch 00037: val_accuracy improved from 0.50199 to 0.50465, saving model to .\\model\\gabor_tone_cnn1.h5\n",
      "95/95 [==============================] - 1s 12ms/step - loss: 0.6933 - accuracy: 0.7186 - val_loss: 1.2998 - val_accuracy: 0.5046\n",
      "Epoch 38/100\n",
      "95/95 [==============================] - ETA: 0s - loss: 0.6928 - accuracy: 0.7206\n",
      "Epoch 00038: val_accuracy did not improve from 0.50465\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 0.6928 - accuracy: 0.7206 - val_loss: 1.3648 - val_accuracy: 0.4781\n",
      "Epoch 39/100\n",
      "93/95 [============================>.] - ETA: 0s - loss: 0.6477 - accuracy: 0.7453\n",
      "Epoch 00039: val_accuracy did not improve from 0.50465\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 0.6487 - accuracy: 0.7445 - val_loss: 1.3454 - val_accuracy: 0.4861\n",
      "Epoch 40/100\n",
      "91/95 [===========================>..] - ETA: 0s - loss: 0.6707 - accuracy: 0.7339\n",
      "Epoch 00040: val_accuracy did not improve from 0.50465\n",
      "95/95 [==============================] - 1s 12ms/step - loss: 0.6691 - accuracy: 0.7349 - val_loss: 1.3568 - val_accuracy: 0.4821\n",
      "Epoch 41/100\n",
      "91/95 [===========================>..] - ETA: 0s - loss: 0.6413 - accuracy: 0.7428\n",
      "Epoch 00041: val_accuracy did not improve from 0.50465\n",
      "95/95 [==============================] - 1s 12ms/step - loss: 0.6443 - accuracy: 0.7412 - val_loss: 1.3683 - val_accuracy: 0.4834\n",
      "Epoch 42/100\n",
      "91/95 [===========================>..] - ETA: 0s - loss: 0.6312 - accuracy: 0.7445\n",
      "Epoch 00042: val_accuracy did not improve from 0.50465\n",
      "95/95 [==============================] - 1s 12ms/step - loss: 0.6293 - accuracy: 0.7452 - val_loss: 1.3692 - val_accuracy: 0.4914\n",
      "Epoch 43/100\n",
      "94/95 [============================>.] - ETA: 0s - loss: 0.6018 - accuracy: 0.7550\n",
      "Epoch 00043: val_accuracy did not improve from 0.50465\n",
      "95/95 [==============================] - 1s 12ms/step - loss: 0.6029 - accuracy: 0.7545 - val_loss: 1.3808 - val_accuracy: 0.4874\n",
      "Epoch 44/100\n",
      "90/95 [===========================>..] - ETA: 0s - loss: 0.6213 - accuracy: 0.7545\n",
      "Epoch 00044: val_accuracy did not improve from 0.50465\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 0.6230 - accuracy: 0.7532 - val_loss: 1.4089 - val_accuracy: 0.4794\n",
      "Epoch 45/100\n",
      "94/95 [============================>.] - ETA: 0s - loss: 0.5664 - accuracy: 0.7822\n",
      "Epoch 00045: val_accuracy did not improve from 0.50465\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 0.5666 - accuracy: 0.7821 - val_loss: 1.4214 - val_accuracy: 0.4794\n",
      "Epoch 46/100\n",
      "94/95 [============================>.] - ETA: 0s - loss: 0.5761 - accuracy: 0.7749\n",
      "Epoch 00046: val_accuracy did not improve from 0.50465\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 0.5758 - accuracy: 0.7751 - val_loss: 1.4354 - val_accuracy: 0.4821\n",
      "Epoch 47/100\n",
      "90/95 [===========================>..] - ETA: 0s - loss: 0.5477 - accuracy: 0.7858\n",
      "Epoch 00047: val_accuracy did not improve from 0.50465\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 0.5485 - accuracy: 0.7837 - val_loss: 1.4386 - val_accuracy: 0.4781\n",
      "Epoch 48/100\n",
      "90/95 [===========================>..] - ETA: 0s - loss: 0.5381 - accuracy: 0.8028\n",
      "Epoch 00048: val_accuracy did not improve from 0.50465\n",
      "95/95 [==============================] - 1s 12ms/step - loss: 0.5361 - accuracy: 0.8033 - val_loss: 1.4277 - val_accuracy: 0.4967\n",
      "Epoch 49/100\n",
      "90/95 [===========================>..] - ETA: 0s - loss: 0.5429 - accuracy: 0.7847\n",
      "Epoch 00049: val_accuracy did not improve from 0.50465\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 0.5451 - accuracy: 0.7831 - val_loss: 1.4369 - val_accuracy: 0.4900\n",
      "Epoch 50/100\n",
      "93/95 [============================>.] - ETA: 0s - loss: 0.5163 - accuracy: 0.8007\n",
      "Epoch 00050: val_accuracy did not improve from 0.50465\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 0.5156 - accuracy: 0.8013 - val_loss: 1.4874 - val_accuracy: 0.4861\n",
      "Epoch 51/100\n",
      "92/95 [============================>.] - ETA: 0s - loss: 0.5029 - accuracy: 0.8071\n",
      "Epoch 00051: val_accuracy did not improve from 0.50465\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 0.5045 - accuracy: 0.8053 - val_loss: 1.5413 - val_accuracy: 0.4568\n",
      "Epoch 52/100\n",
      "91/95 [===========================>..] - ETA: 0s - loss: 0.4869 - accuracy: 0.8156\n",
      "Epoch 00052: val_accuracy did not improve from 0.50465\n",
      "95/95 [==============================] - 1s 12ms/step - loss: 0.4824 - accuracy: 0.8169 - val_loss: 1.5272 - val_accuracy: 0.4635\n",
      "Epoch 53/100\n",
      "93/95 [============================>.] - ETA: 0s - loss: 0.4727 - accuracy: 0.8192\n",
      "Epoch 00053: val_accuracy did not improve from 0.50465\n",
      "95/95 [==============================] - 1s 12ms/step - loss: 0.4713 - accuracy: 0.8199 - val_loss: 1.4791 - val_accuracy: 0.4821\n",
      "Epoch 54/100\n",
      "91/95 [===========================>..] - ETA: 0s - loss: 0.4623 - accuracy: 0.8286\n",
      "Epoch 00054: val_accuracy did not improve from 0.50465\n",
      "95/95 [==============================] - 1s 12ms/step - loss: 0.4658 - accuracy: 0.8272 - val_loss: 1.5156 - val_accuracy: 0.4781\n",
      "Epoch 55/100\n",
      "94/95 [============================>.] - ETA: 0s - loss: 0.4770 - accuracy: 0.8092\n",
      "Epoch 00055: val_accuracy did not improve from 0.50465\n",
      "95/95 [==============================] - 1s 12ms/step - loss: 0.4767 - accuracy: 0.8093 - val_loss: 1.5471 - val_accuracy: 0.4781\n",
      "Epoch 56/100\n",
      "90/95 [===========================>..] - ETA: 0s - loss: 0.4371 - accuracy: 0.8365\n",
      "Epoch 00056: val_accuracy did not improve from 0.50465\n",
      "95/95 [==============================] - 1s 12ms/step - loss: 0.4374 - accuracy: 0.8362 - val_loss: 1.5709 - val_accuracy: 0.4608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100\n",
      "94/95 [============================>.] - ETA: 0s - loss: 0.4478 - accuracy: 0.8245\n",
      "Epoch 00057: val_accuracy did not improve from 0.50465\n",
      "95/95 [==============================] - 1s 12ms/step - loss: 0.4477 - accuracy: 0.8246 - val_loss: 1.5702 - val_accuracy: 0.4622\n",
      "Epoch 58/100\n",
      "94/95 [============================>.] - ETA: 0s - loss: 0.4313 - accuracy: 0.8388\n",
      "Epoch 00058: val_accuracy did not improve from 0.50465\n",
      "95/95 [==============================] - 1s 12ms/step - loss: 0.4313 - accuracy: 0.8389 - val_loss: 1.5487 - val_accuracy: 0.4861\n",
      "Epoch 59/100\n",
      "91/95 [===========================>..] - ETA: 0s - loss: 0.4122 - accuracy: 0.8400\n",
      "Epoch 00059: val_accuracy did not improve from 0.50465\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 0.4142 - accuracy: 0.8395 - val_loss: 1.5644 - val_accuracy: 0.4648\n",
      "Epoch 60/100\n",
      "91/95 [===========================>..] - ETA: 0s - loss: 0.4061 - accuracy: 0.8492\n",
      "Epoch 00060: val_accuracy did not improve from 0.50465\n",
      "95/95 [==============================] - 1s 12ms/step - loss: 0.4078 - accuracy: 0.8495 - val_loss: 1.5760 - val_accuracy: 0.4900\n",
      "Epoch 61/100\n",
      "92/95 [============================>.] - ETA: 0s - loss: 0.4047 - accuracy: 0.8400\n",
      "Epoch 00061: val_accuracy did not improve from 0.50465\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 0.4025 - accuracy: 0.8419 - val_loss: 1.5855 - val_accuracy: 0.4807\n",
      "Epoch 62/100\n",
      "91/95 [===========================>..] - ETA: 0s - loss: 0.3718 - accuracy: 0.8565\n",
      "Epoch 00062: val_accuracy did not improve from 0.50465\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 0.3715 - accuracy: 0.8561 - val_loss: 1.6566 - val_accuracy: 0.4635\n",
      "Epoch 63/100\n",
      "90/95 [===========================>..] - ETA: 0s - loss: 0.4218 - accuracy: 0.8375\n",
      "Epoch 00063: val_accuracy did not improve from 0.50465\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 0.4207 - accuracy: 0.8375 - val_loss: 1.6076 - val_accuracy: 0.4754\n",
      "Epoch 64/100\n",
      "92/95 [============================>.] - ETA: 0s - loss: 0.3859 - accuracy: 0.8519\n",
      "Epoch 00064: val_accuracy did not improve from 0.50465\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 0.3877 - accuracy: 0.8525 - val_loss: 1.6351 - val_accuracy: 0.4887\n",
      "Epoch 65/100\n",
      "93/95 [============================>.] - ETA: 0s - loss: 0.3666 - accuracy: 0.8525\n",
      "Epoch 00065: val_accuracy did not improve from 0.50465\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 0.3660 - accuracy: 0.8528 - val_loss: 1.6354 - val_accuracy: 0.4900\n",
      "Epoch 66/100\n",
      "92/95 [============================>.] - ETA: 0s - loss: 0.3542 - accuracy: 0.8648\n",
      "Epoch 00066: val_accuracy did not improve from 0.50465\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 0.3510 - accuracy: 0.8664 - val_loss: 1.6892 - val_accuracy: 0.4728\n",
      "Epoch 67/100\n",
      "91/95 [===========================>..] - ETA: 0s - loss: 0.3466 - accuracy: 0.8750\n",
      "Epoch 00067: val_accuracy did not improve from 0.50465\n",
      "95/95 [==============================] - 1s 12ms/step - loss: 0.3472 - accuracy: 0.8734 - val_loss: 1.6960 - val_accuracy: 0.4927\n",
      "Epoch 68/100\n",
      "91/95 [===========================>..] - ETA: 0s - loss: 0.3379 - accuracy: 0.8716\n",
      "Epoch 00068: val_accuracy did not improve from 0.50465\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 0.3366 - accuracy: 0.8728 - val_loss: 1.7575 - val_accuracy: 0.4754\n",
      "Epoch 69/100\n",
      "91/95 [===========================>..] - ETA: 0s - loss: 0.3575 - accuracy: 0.8692\n",
      "Epoch 00069: val_accuracy did not improve from 0.50465\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 0.3557 - accuracy: 0.8701 - val_loss: 1.6956 - val_accuracy: 0.4714\n",
      "Epoch 70/100\n",
      "91/95 [===========================>..] - ETA: 0s - loss: 0.3128 - accuracy: 0.8808\n",
      "Epoch 00070: val_accuracy did not improve from 0.50465\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 0.3113 - accuracy: 0.8811 - val_loss: 1.7323 - val_accuracy: 0.4688\n",
      "Epoch 71/100\n",
      "92/95 [============================>.] - ETA: 0s - loss: 0.3240 - accuracy: 0.8815\n",
      "Epoch 00071: val_accuracy did not improve from 0.50465\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 0.3240 - accuracy: 0.8817 - val_loss: 1.7495 - val_accuracy: 0.4900\n",
      "Epoch 72/100\n",
      "91/95 [===========================>..] - ETA: 0s - loss: 0.3089 - accuracy: 0.8846\n",
      "Epoch 00072: val_accuracy did not improve from 0.50465\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 0.3070 - accuracy: 0.8857 - val_loss: 1.8180 - val_accuracy: 0.4529\n",
      "Epoch 73/100\n",
      "91/95 [===========================>..] - ETA: 0s - loss: 0.3066 - accuracy: 0.8791\n",
      "Epoch 00073: val_accuracy did not improve from 0.50465\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 0.3070 - accuracy: 0.8797 - val_loss: 1.7808 - val_accuracy: 0.4714\n",
      "Epoch 74/100\n",
      "92/95 [============================>.] - ETA: 0s - loss: 0.2905 - accuracy: 0.8893\n",
      "Epoch 00074: val_accuracy did not improve from 0.50465\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 0.2914 - accuracy: 0.8887 - val_loss: 1.8362 - val_accuracy: 0.4635\n",
      "Epoch 75/100\n",
      "91/95 [===========================>..] - ETA: 0s - loss: 0.2666 - accuracy: 0.9028\n",
      "Epoch 00075: val_accuracy did not improve from 0.50465\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 0.2676 - accuracy: 0.9017 - val_loss: 1.9000 - val_accuracy: 0.4768\n",
      "Epoch 76/100\n",
      "91/95 [===========================>..] - ETA: 0s - loss: 0.2846 - accuracy: 0.8891\n",
      "Epoch 00076: val_accuracy did not improve from 0.50465\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 0.2826 - accuracy: 0.8907 - val_loss: 1.8346 - val_accuracy: 0.4675\n",
      "Epoch 77/100\n",
      "91/95 [===========================>..] - ETA: 0s - loss: 0.2919 - accuracy: 0.8898\n",
      "Epoch 00077: val_accuracy did not improve from 0.50465\n",
      "95/95 [==============================] - 1s 12ms/step - loss: 0.2930 - accuracy: 0.8890 - val_loss: 1.8326 - val_accuracy: 0.4887\n",
      "Epoch 78/100\n",
      "91/95 [===========================>..] - ETA: 0s - loss: 0.2887 - accuracy: 0.8915\n",
      "Epoch 00078: val_accuracy did not improve from 0.50465\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 0.2892 - accuracy: 0.8910 - val_loss: 1.7838 - val_accuracy: 0.4874\n",
      "Epoch 79/100\n",
      "92/95 [============================>.] - ETA: 0s - loss: 0.2605 - accuracy: 0.9086\n",
      "Epoch 00079: val_accuracy did not improve from 0.50465\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 0.2595 - accuracy: 0.9093 - val_loss: 1.8407 - val_accuracy: 0.4940\n",
      "Epoch 80/100\n",
      "92/95 [============================>.] - ETA: 0s - loss: 0.2581 - accuracy: 0.9018\n",
      "Epoch 00080: val_accuracy did not improve from 0.50465\n",
      "95/95 [==============================] - 1s 12ms/step - loss: 0.2599 - accuracy: 0.9013 - val_loss: 1.8378 - val_accuracy: 0.4900\n",
      "Epoch 81/100\n",
      "92/95 [============================>.] - ETA: 0s - loss: 0.2568 - accuracy: 0.9073\n",
      "Epoch 00081: val_accuracy did not improve from 0.50465\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 0.2574 - accuracy: 0.9063 - val_loss: 1.8431 - val_accuracy: 0.4807\n",
      "Epoch 82/100\n",
      "95/95 [==============================] - ETA: 0s - loss: 0.2473 - accuracy: 0.9100\n",
      "Epoch 00082: val_accuracy did not improve from 0.50465\n",
      "95/95 [==============================] - 1s 12ms/step - loss: 0.2473 - accuracy: 0.9100 - val_loss: 1.9166 - val_accuracy: 0.4728\n",
      "Epoch 83/100\n",
      "90/95 [===========================>..] - ETA: 0s - loss: 0.2395 - accuracy: 0.9125\n",
      "Epoch 00083: val_accuracy did not improve from 0.50465\n",
      "95/95 [==============================] - 1s 12ms/step - loss: 0.2403 - accuracy: 0.9120 - val_loss: 1.9301 - val_accuracy: 0.4555\n",
      "Epoch 84/100\n",
      "92/95 [============================>.] - ETA: 0s - loss: 0.2446 - accuracy: 0.9096\n",
      "Epoch 00084: val_accuracy did not improve from 0.50465\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 0.2428 - accuracy: 0.9113 - val_loss: 1.9742 - val_accuracy: 0.4529\n",
      "Epoch 85/100\n",
      "92/95 [============================>.] - ETA: 0s - loss: 0.3644 - accuracy: 0.8682\n",
      "Epoch 00085: val_accuracy did not improve from 0.50465\n",
      "95/95 [==============================] - 1s 12ms/step - loss: 0.3640 - accuracy: 0.8684 - val_loss: 1.8621 - val_accuracy: 0.4489\n",
      "Epoch 86/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91/95 [===========================>..] - ETA: 0s - loss: 0.2557 - accuracy: 0.9014\n",
      "Epoch 00086: val_accuracy did not improve from 0.50465\n",
      "95/95 [==============================] - 1s 12ms/step - loss: 0.2558 - accuracy: 0.9010 - val_loss: 1.9258 - val_accuracy: 0.4515\n",
      "Epoch 87/100\n",
      "92/95 [============================>.] - ETA: 0s - loss: 0.2292 - accuracy: 0.9164\n",
      "Epoch 00087: val_accuracy did not improve from 0.50465\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 0.2284 - accuracy: 0.9166 - val_loss: 1.9031 - val_accuracy: 0.4502\n",
      "Epoch 88/100\n",
      "91/95 [===========================>..] - ETA: 0s - loss: 0.2171 - accuracy: 0.9255\n",
      "Epoch 00088: val_accuracy did not improve from 0.50465\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 0.2172 - accuracy: 0.9266 - val_loss: 1.9186 - val_accuracy: 0.4542\n",
      "Epoch 89/100\n",
      "92/95 [============================>.] - ETA: 0s - loss: 0.2391 - accuracy: 0.9120\n",
      "Epoch 00089: val_accuracy did not improve from 0.50465\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 0.2426 - accuracy: 0.9100 - val_loss: 1.9230 - val_accuracy: 0.4622\n",
      "Epoch 90/100\n",
      "92/95 [============================>.] - ETA: 0s - loss: 0.1989 - accuracy: 0.9351\n",
      "Epoch 00090: val_accuracy did not improve from 0.50465\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 0.1993 - accuracy: 0.9346 - val_loss: 1.9494 - val_accuracy: 0.4807\n",
      "Epoch 91/100\n",
      "91/95 [===========================>..] - ETA: 0s - loss: 0.2053 - accuracy: 0.9337\n",
      "Epoch 00091: val_accuracy did not improve from 0.50465\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 0.2084 - accuracy: 0.9326 - val_loss: 1.9758 - val_accuracy: 0.4741\n",
      "Epoch 92/100\n",
      "91/95 [===========================>..] - ETA: 0s - loss: 0.1958 - accuracy: 0.9317\n",
      "Epoch 00092: val_accuracy did not improve from 0.50465\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 0.1953 - accuracy: 0.9326 - val_loss: 1.9719 - val_accuracy: 0.4768\n",
      "Epoch 93/100\n",
      "91/95 [===========================>..] - ETA: 0s - loss: 0.2023 - accuracy: 0.9282\n",
      "Epoch 00093: val_accuracy did not improve from 0.50465\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 0.2039 - accuracy: 0.9276 - val_loss: 1.9583 - val_accuracy: 0.4821\n",
      "Epoch 94/100\n",
      "91/95 [===========================>..] - ETA: 0s - loss: 0.2021 - accuracy: 0.9299\n",
      "Epoch 00094: val_accuracy did not improve from 0.50465\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 0.1995 - accuracy: 0.9306 - val_loss: 2.0003 - val_accuracy: 0.4794\n",
      "Epoch 95/100\n",
      "90/95 [===========================>..] - ETA: 0s - loss: 0.2022 - accuracy: 0.9274\n",
      "Epoch 00095: val_accuracy did not improve from 0.50465\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 0.2014 - accuracy: 0.9279 - val_loss: 1.9791 - val_accuracy: 0.4648\n",
      "Epoch 96/100\n",
      "92/95 [============================>.] - ETA: 0s - loss: 0.2063 - accuracy: 0.9232\n",
      "Epoch 00096: val_accuracy did not improve from 0.50465\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 0.2061 - accuracy: 0.9229 - val_loss: 1.9589 - val_accuracy: 0.4754\n",
      "Epoch 97/100\n",
      "90/95 [===========================>..] - ETA: 0s - loss: 0.1954 - accuracy: 0.9323\n",
      "Epoch 00097: val_accuracy did not improve from 0.50465\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 0.1976 - accuracy: 0.9306 - val_loss: 2.0069 - val_accuracy: 0.4661\n",
      "Epoch 98/100\n",
      "91/95 [===========================>..] - ETA: 0s - loss: 0.2777 - accuracy: 0.8994\n",
      "Epoch 00098: val_accuracy did not improve from 0.50465\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 0.2735 - accuracy: 0.9010 - val_loss: 2.0601 - val_accuracy: 0.4582\n",
      "Epoch 99/100\n",
      "91/95 [===========================>..] - ETA: 0s - loss: 0.2145 - accuracy: 0.9245\n",
      "Epoch 00099: val_accuracy did not improve from 0.50465\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 0.2142 - accuracy: 0.9236 - val_loss: 2.0286 - val_accuracy: 0.4741\n",
      "Epoch 100/100\n",
      "92/95 [============================>.] - ETA: 0s - loss: 0.1810 - accuracy: 0.9348\n",
      "Epoch 00100: val_accuracy did not improve from 0.50465\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 0.1812 - accuracy: 0.9346 - val_loss: 2.0274 - val_accuracy: 0.4821\n",
      "time= 113.97724318504333\n"
     ]
    }
   ],
   "source": [
    "a=time.time()\n",
    "model.compile(optimizer=adam,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "#plot_model(model,'model_tune.jpg',show_shapes=True)\n",
    "save_model=kcallbacks.ModelCheckpoint(filepath=MODEL_SAVE_PATH,monitor='val_accuracy',verbose=1,save_best_only=True)\n",
    "callback_list=[save_model]\n",
    "history=model.fit(x=X_train,y=y_train,batch_size=BATCH_SIZE,epochs=EPOCHS,validation_split=0.2,\n",
    "                  callbacks=callback_list)\n",
    "b=time.time()\n",
    "print(\"time=\",b-a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import load_model\n",
    "model=load_model('.\\model\\gabor_tone_cnn1.h5')\n",
    "# history=model.fit(x=X_train,y=y_train,batch_size=BATCH_SIZE,epochs=EPOCHS,validation_data=(X_test,y_test),\n",
    "#                   callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from help import plot_curve,plot_curve_with_smooth\n",
    "plot_curve(history)\n",
    "plot_curve_with_smooth(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## confusion_metrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1d80517edd8>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "y_pred=model.predict(X_test)\n",
    "y_pred=np.argmax(y_pred,axis=-1).ravel()\n",
    "y_true=np.argmax(y_test,axis=-1).ravel()\n",
    "con_matrix=confusion_matrix(y_true=y_true,y_pred=y_pred)\n",
    "# def plot_confusion_matrix(confusion_mat):\n",
    "#     plt.imshow(confusion_mat)\n",
    "#     plt.title('Confusion Matrix')\n",
    "#     plt.colorbar()\n",
    "labels = [1,2,3,4]\n",
    "#     tick_marks = np.arange(len(labels))\n",
    "#     plt.xticks(tick_marks, labels)\n",
    "#     plt.yticks(tick_marks, labels)\n",
    "#     plt.ylabel('True Label')\n",
    "#     plt.xlabel('Predicted Label')\n",
    "#     plt.grid(False)\n",
    "#     plt.show() \n",
    "# plot_confusion_matrix(con_matrix)\n",
    "import seaborn as sn\n",
    "\n",
    "df_cm = pd.DataFrame(con_matrix,\n",
    "                     index = [i for i in list(labels)],\n",
    "                     columns = [i for i in list(labels)])\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(df_cm, annot=True, fmt='.20g',cmap=\"BuPu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## classfication_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.3941    0.4398    0.4157       241\n",
      "           2     0.3384    0.3300    0.3342       203\n",
      "           3     0.5890    0.4886    0.5342       264\n",
      "           4     0.5059    0.5536    0.5287       233\n",
      "\n",
      "   micro avg     0.4580    0.4580    0.4580       941\n",
      "   macro avg     0.4568    0.4530    0.4532       941\n",
      "weighted avg     0.4644    0.4580    0.4593       941\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_names = ['1', '2', '3','4']\n",
    "print(classification_report(y_true,y_pred,digits=4,labels=[0,1,2,3],target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Che1.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 35, 7)\n",
      "Predict Label:3\n"
     ]
    }
   ],
   "source": [
    "temp = np.load('che1.npy')\n",
    "m,n = temp.shape\n",
    "            #print(m)\n",
    "if m <=35:\n",
    "    left = 36 - m\n",
    "    temp =np.append(temp,(np.zeros([left,n])),axis=0)\n",
    "test =[]\n",
    "test.append(temp[:35,])\n",
    "test = np.array(test)\n",
    "print(test.shape)\n",
    "pred=model.predict(test)\n",
    "pred=np.argmax(pred,axis=1)[0]\n",
    "print('Predict Label:%s'%(CLASSES_LIST[pred]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "import sklearn\n",
    "def get_mfcc_feature(path,maxlen=50):\n",
    "    x, sr = librosa.load(path)\n",
    "    # extract mfcc\n",
    "    mfcc = librosa.feature.mfcc(x, sr=sr)\n",
    "    # Center to the mean and component wise scale to unit variance\n",
    "    mfcc = sklearn.preprocessing.scale(mfcc, axis=1)\n",
    "    mfcc_diff = np.diff(mfcc, axis=0)\n",
    "    mfcc_diff_diff = np.diff(mfcc_diff, axis=0)\n",
    "    # concatenate mfcc,mfcc_diff and mfcc_diff_diff\n",
    "    mfccs = np.concatenate([mfcc, mfcc_diff, mfcc_diff_diff], axis=0).T\n",
    "    #pad the mfccs_feature\n",
    "    if mfccs.shape[0]<maxlen:\n",
    "        mfccs=np.concatenate([mfccs,np.zeros((maxlen-mfccs.shape[0],57))],axis=0)\n",
    "    return mfccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "wav_file='./voice_data/person10_time1_group1_ba_1.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict Label:1\n"
     ]
    }
   ],
   "source": [
    "test=get_mfcc_feature(path=wav_file)\n",
    "test=np.expand_dims(test,axis=0)\n",
    "pred=model.predict(test)\n",
    "pred=np.argmax(pred,axis=1)[0]\n",
    "print('Predict Label:%s'%(CLASSES_LIST[pred]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 35, 7)\n",
      "Predict Label:4\n"
     ]
    }
   ],
   "source": [
    "temp = np.load('./gabor/person2_time1_group1_da_4.npy')\n",
    "m,n = temp.shape\n",
    "            #print(m)\n",
    "if m <=35:\n",
    "    left = 36 - m\n",
    "    temp =np.append(temp,(np.zeros([left,n])),axis=0)\n",
    "test =[]\n",
    "test.append(temp[:35,])\n",
    "test = np.array(test)\n",
    "print(test.shape)\n",
    "pred=model.predict(test)\n",
    "pred=np.argmax(pred,axis=1)[0]\n",
    "print('Predict Label:%s'%(CLASSES_LIST[pred]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable-length int sequences.\n",
    "query_input = tf.keras.Input(shape=(None,), dtype='int32')\n",
    "value_input = tf.keras.Input(shape=(None,), dtype='int32')\n",
    "\n",
    "# Embedding lookup.\n",
    "token_embedding = tf.keras.layers.Embedding(input_dim=1000, output_dim=64)\n",
    "# Query embeddings of shape [batch_size, Tq, dimension].\n",
    "query_embeddings = token_embedding(query_input)\n",
    "# Value embeddings of shape [batch_size, Tv, dimension].\n",
    "value_embeddings = token_embedding(value_input)\n",
    "\n",
    "# CNN layer.\n",
    "cnn_layer = tf.keras.layers.Conv1D(\n",
    "    filters=100,\n",
    "    kernel_size=4,\n",
    "    # Use 'same' padding so outputs have the same shape as inputs.\n",
    "    padding='same')\n",
    "# Query encoding of shape [batch_size, Tq, filters].\n",
    "query_seq_encoding = cnn_layer(query_embeddings)\n",
    "# Value encoding of shape [batch_size, Tv, filters].\n",
    "value_seq_encoding = cnn_layer(value_embeddings)\n",
    "\n",
    "# Query-value attention of shape [batch_size, Tq, filters].\n",
    "query_value_attention_seq = tf.keras.layers.Attention()(\n",
    "    [query_seq_encoding, value_seq_encoding])\n",
    "\n",
    "# Reduce over the sequence axis to produce encodings of shape\n",
    "# [batch_size, filters].\n",
    "query_encoding = tf.keras.layers.GlobalAveragePooling1D()(\n",
    "    query_seq_encoding)\n",
    "query_value_attention = tf.keras.layers.GlobalAveragePooling1D()(\n",
    "    query_value_attention_seq)\n",
    "\n",
    "# Concatenate query and document encodings to produce a DNN input layer.\n",
    "input_layer = tf.keras.layers.Concatenate()(\n",
    "    [query_encoding, query_value_attention])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
